{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/01/15 11:11:08 WARN Utils: Your hostname, CPC-12806 resolves to a loopback address: 127.0.1.1; using 172.26.242.248 instead (on interface eth0)\n",
      "25/01/15 11:11:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/arthur/minio-python-client/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/arthur/.ivy2/cache\n",
      "The jars for the packages stored in: /home/arthur/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0291f313-9446-425d-84a7-852a6626dd50;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 296ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0291f313-9446-425d-84a7-852a6626dd50\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/9ms)\n",
      "25/01/15 11:11:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "            \"--packages org.apache.hadoop:hadoop-aws:3.3.4 pyspark-shell\"\n",
    "        )\n",
    "\n",
    "# Inicializar a sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ParquetReaderExample\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ[\"MINIO_ENDPOINT\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"MINIO_ACCESS_KEY\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"MINIO_SECRET_KEY\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .getOrCreate()  # noqa: E501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------------+\n",
      "| id|  categoria|             valor|\n",
      "+---+-----------+------------------+\n",
      "|  0|Categoria_4| 141.5955612049314|\n",
      "|  1|Categoria_2|232.88568243203932|\n",
      "|  2|Categoria_1| 80.34393746959496|\n",
      "|  3|Categoria_5|359.03091052980693|\n",
      "|  4|Categoria_5|150.48462611896738|\n",
      "+---+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criar um DataFrame com dados aleatórios simulando um cenário real\n",
    "data = [(i, f\"Categoria_{random.randint(1, 5)}\", random.uniform(10.0, 500.0)) for i in range(200)]\n",
    "columns = [\"id\", \"categoria\", \"valor\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Exibir os primeiros registros\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/15 11:14:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/01/15 11:14:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/01/15 11:14:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/01/15 11:14:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/01/15 11:14:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/01/15 11:14:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/01/15 11:14:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/01/15 11:14:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/01/15 11:14:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Salvar o DataFrame no MinIO em formato Parquet particionado pela coluna 'categoria'\n",
    "output_path = \"s3a://sandbox/pyspark_example/\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------------+\n",
      "| id|  categoria|             valor|\n",
      "+---+-----------+------------------+\n",
      "|176|Categoria_2|497.30971569959235|\n",
      "|177|Categoria_3| 325.7741928192519|\n",
      "|178|Categoria_2|116.64221565713207|\n",
      "|179|Categoria_2|490.02376605378424|\n",
      "|180|Categoria_3|456.00108822969884|\n",
      "+---+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Caminho para os arquivos Parquet\n",
    "parquet_path = \"s3a://sandbox/pyspark_example/*.parquet\"\n",
    "\n",
    "# Ler todos os arquivos Parquet\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Exibir os primeiros registros\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------------+\n",
      "| id|  categoria|             valor|\n",
      "+---+-----------+------------------+\n",
      "|176|Categoria_2|497.30971569959235|\n",
      "|177|Categoria_3| 325.7741928192519|\n",
      "|178|Categoria_2|116.64221565713207|\n",
      "|179|Categoria_2|490.02376605378424|\n",
      "|180|Categoria_3|456.00108822969884|\n",
      "+---+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformação simples: filtrar registros onde a coluna 'valor' > 100\n",
    "df_filtered = df.filter(df[\"valor\"] > 100)\n",
    "\n",
    "# Exibir o DataFrame filtrado\n",
    "df_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------------+\n",
      "| id|  categoria|             valor|\n",
      "+---+-----------+------------------+\n",
      "|176|Categoria_2|497.30971569959235|\n",
      "|177|Categoria_3| 325.7741928192519|\n",
      "|178|Categoria_2|116.64221565713207|\n",
      "|179|Categoria_2|490.02376605378424|\n",
      "|180|Categoria_3|456.00108822969884|\n",
      "+---+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Registrar o DataFrame como uma tabela temporária\n",
    "df.createOrReplaceTempView(\"dados\")\n",
    "\n",
    "# Executar a consulta SQL para filtrar registros com valor > 100\n",
    "df_filtered_sql = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM dados\n",
    "    WHERE valor > 100\n",
    "\"\"\")\n",
    "\n",
    "# Exibir o resultado da filtragem\n",
    "df_filtered_sql.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
